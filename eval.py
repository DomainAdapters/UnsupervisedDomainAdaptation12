# -*- coding: utf-8 -*-
"""eval.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hPmT6tw64lRZU4wW6HJ1aMNWH8rWdZqV
"""

import torch
from torch.utils.tensorboard import SummaryWriter
from dataloader import MyDataset, MyTransformer
from torch.utils.data import DataLoader
import torch.nn as nn
from utils import EvaluationManager, get_num_correct
from net import Obj_classifier, Resnet_extractor, Obj_classifier, Rot_classifier, Rot_regressor

variation = False


# here we load an already existing model
rgb_extractor1 = Resnet_extractor()
depth_extrector1 = Resnet_extractor()
obj_classifier1 = Obj_classifier()
#rot_classifier1 = Rot_classifier()
if variation == False:
  rgb_save_name = 'rgb_extrector.pt'
  depth_save_name = 'depth_extrector.pt'
  obj_save_name = 'obj_classifier.pt'
else:
  rgb_save_name = 'rgb_extrector_var.pt'
  depth_save_name = 'depth_extrector_var.pt'
  obj_save_name = 'obj_classifier_var.pt'


path = F"./model_saved/{rgb_save_name}"
rgb_extractor1.load_state_dict(torch.load(path))
path = F"./model_saved/{depth_extrector1}"
depth_extrector1.load_state_dict(torch.load(path))
path = F"./model_saved/{obj_classifier1}"
obj_classifier1.load_state_dict(torch.load(path))


tb = SummaryWriter('./log')
device = torch.device('cuda')
net_list = [rgb_extractor1, depth_extrector1, obj_classifier1]
entropy_loss = nn.CrossEntropyLoss()

rod_evaluation = MyDataset("./ROD-synROD/ROD/wrgbd_40k-split_sync.txt", fold_name = "ROD")

Rod_evaluation_loader = DataLoader( 
            rod_evaluation,
            shuffle=True,                    # rod for evaluation of the whole model
            batch_size=62,
            num_workers=1,
            )


with EvaluationManager(net_list):
  Rod_evaluation_loader_iter = iter(Rod_evaluation_loader) 
  correct = 0.0
  num_predictions = 0.0
  val_loss = 0.0
  # for i in range(0, 1000, 64):
  for tmp in Rod_evaluation_loader_iter:
    #tmp = Rod_evaluation_loader_iter.get_next()
    rgb_img = tmp[0].to(device)
    depth_img = tmp[1].to(device)
    label = tmp[2].to(device)

    rgb_out, _ = rgb_extractor1(rgb_img)
    depth_out, _ = depth_extrector1(depth_img)
    out_concat = torch.cat((rgb_out, depth_out), 1)
    pred_labels = obj_classifier1(out_concat) 
    pred_labels =   pred_labels.type(torch.float32)

    val_loss += entropy_loss(pred_labels, label).item() 
    correct +=get_num_correct( pred_labels, label) 
    num_predictions += rgb_img.shape[0]
    del rgb_img, depth_img, label, pred_labels,rgb_out, depth_out, out_concat

  accuracy = correct/ num_predictions
  actual_loss = val_loss/ num_predictions   # need to confirm form prof, either to divide by batch size or total no of imgs
  #print("Epoch: {} -- validation of source-- accuracy: {}".format(epoch, accuracy))
  print("Accuracy: ", accuracy)

#eval Variation
from tqdm.notebook import tqdm
import torch

#Test accuracy rgb and depth
def test_acc_rgbd(net, test, DEVICE):
  net.train(False) 
  running_corrects_label = 0
  Counter=0

  for rgb, depth, labels in tqdm(test):
    rgb = rgb.to(DEVICE)
    depth = depth.to(DEVICE)
    labels = labels.to(DEVICE)

    lab = net(rgb, depth)
    
    _, preds = torch.max(lab.data, 1)

    running_corrects_label += torch.sum(preds == labels.data).data.item()

  val = running_corrects_label / float(len(test.dataset))
  print(val)
  return val